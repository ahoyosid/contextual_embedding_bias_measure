{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply debias BERT by optimizing the log odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "from overrides import overrides\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_utils import Config, BertPreprocessor\n",
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    "    batch_size=64,\n",
    "    testing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "\n",
    "def flatten(x: List[List[T]]) -> List[T]:\n",
    "        return [item for sublist in x for item in sublist]\n",
    "\n",
    "token_indexer = PretrainedBertIndexer(\n",
    "    pretrained_model=config.model_type,\n",
    "    max_pieces=config.max_seq_len,\n",
    "    do_lowercase=True,\n",
    " )\n",
    "\n",
    "def tokenizer(s: str):\n",
    "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from allennlp.data import DatasetReader, Instance, Token\n",
    "from allennlp.data.fields import (TextField, SequenceLabelField, LabelField, \n",
    "                                  MetadataField, ArrayField)\n",
    "\n",
    "class LongArrayField(ArrayField):\n",
    "    @overrides\n",
    "    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n",
    "        tensor = torch.from_numpy(self.array)\n",
    "        return tensor\n",
    "\n",
    "class DebiasingDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer, token_indexers) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers\n",
    "        self.vocab = token_indexers[\"tokens\"].vocab\n",
    "\n",
    "    def _proc(self, x):\n",
    "        if x == \"[MASK]\": return x\n",
    "        else: return x.lower()\n",
    "        \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[str], w1: str, w2: str) -> Instance:\n",
    "        fields = {}\n",
    "        fields[\"input\"] = TextField([Token(self._proc(x)) for x in tokens],\n",
    "                                   self.token_indexers)        \n",
    "        # take [CLS] token into account\n",
    "        fields[\"mask_positions\"] = LongArrayField(np.array(tokens.index(\"[MASK]\") + 1, dtype=np.int64),\n",
    "                                             )\n",
    "        fields[\"target_ids\"] = LongArrayField(np.array([\n",
    "            self.vocab[w1], self.vocab[w2],\n",
    "        ], dtype=np.int64))\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path, \"rt\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                sentence, w1, w2 = row\n",
    "                yield self.text_to_instance(\n",
    "                    self.tokenizer(sentence), w1, w2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DebiasingDatasetReader(tokenizer=tokenizer, \n",
    "                                token_indexers={\"tokens\": token_indexer})\n",
    "train_ds, val_ds = (reader.read(DATA_ROOT / fname) for fname in [\"sample.csv\", \"sample.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "iterator = BucketIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        biggest_batch_first=config.testing,\n",
    "        sorting_keys=[(\"input\", \"num_tokens\")],\n",
    "    )\n",
    "iterator.index_with(Vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "masked_lm = BertForMaskedLM.from_pretrained(config.model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm(batch[\"input\"][\"tokens\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(x, y): return ((x - y) ** 2).mean()\n",
    "\n",
    "class BiasLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns the deviation of the log odds ratio from its desired value.\n",
    "    Denoting the probs as p and q there are several options available:\n",
    "        - MSE(log p, log q)\n",
    "        - Max-margin loss\n",
    "    TODO: Add option to set the optimal log odds ratio\n",
    "    \"\"\"\n",
    "    def __init__(self, loss_func: Callable=mse_loss):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "    def forward(self, logits: torch.FloatTensor, # (batch, seq, V)\n",
    "                mask_positions: torch.LongTensor, # (batch, )\n",
    "                target_ids: torch.LongTensor, # (batch, seq, 2)\n",
    "               ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        input_ids: Numericalized tokens\n",
    "        mask_position: Positions of mask tokens\n",
    "        target_ids: Ids of target tokens to compute log odds on\n",
    "        \"\"\"\n",
    "        bs, seq = logits.size(0), logits.size(1)\n",
    "\n",
    "        # Gather the logits for each target id\n",
    "        # TODO: More efficient implementation?\n",
    "        # Gather copies the data to create a new tensor which we would rather avoid        \n",
    "        sel = target_ids.unsqueeze(1).expand(bs, seq, 2)\n",
    "        target_logits = logits.gather(2, sel) # (batch, seq, 2)\n",
    "        \n",
    "        # Gather the logits for each masked position in the sequence\n",
    "        sel = (mask_positions.unsqueeze(1)\n",
    "                .unsqueeze(2).expand(bs, 1, 2)) # (batch, 1, 2)\n",
    "        target_logits_at_masked_positions = target_logits.gather(1, sel).squeeze(1) # (batch, 2)\n",
    "        \n",
    "        return self.loss_func(\n",
    "            target_logits_at_masked_positions[:, 0], # male logits\n",
    "            target_logits_at_masked_positions[:, 1], # female logits\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The allennlp model (for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "\n",
    "class BERT(Model):\n",
    "    def __init__(self, vocab, bert_for_masked_lm):\n",
    "        super().__init__(vocab)\n",
    "        self.bert_for_masked_lm = bert_for_masked_lm\n",
    "        self.loss = BiasLoss()\n",
    "    \n",
    "    def forward(self, \n",
    "                input: TensorDict,\n",
    "                mask_positions: torch.LongTensor,\n",
    "                target_ids: torch.LongTensor,\n",
    "            ) -> TensorDict:\n",
    "        logits = self.bert_for_masked_lm(input[\"tokens\"])\n",
    "        out_dict = {\"loss\": self.loss(logits, mask_positions, target_ids)}\n",
    "        out_dict[\"logits\"] = logits\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT(vocab, masked_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import Trainer\n",
    "\n",
    "optimizer = torch.optim.Adam(masked_lm.parameters(), lr=0.01)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    serialization_dir=None,\n",
    "    cuda_device=0 if torch.cuda.is_available() else -1,\n",
    "    num_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
