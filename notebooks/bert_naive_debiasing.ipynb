{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply debias BERT by optimizing the log odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "from overrides import overrides\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from bert_utils import Config, BertPreprocessor\n",
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    "    batch_size=64,\n",
    "    consistency_weight=1.,\n",
    "    prior_precomputed=True,\n",
    "    testing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\")\n",
    "MODEL_SAVE_DIR = Path(\"../weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the model in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "masked_lm = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "masked_lm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "\n",
    "def flatten(x: List[List[T]]) -> List[T]:\n",
    "        return [item for sublist in x for item in sublist]\n",
    "\n",
    "token_indexer = PretrainedBertIndexer(\n",
    "    pretrained_model=config.model_type,\n",
    "    max_pieces=config.max_seq_len,\n",
    "    do_lowercase=True,\n",
    " )\n",
    "\n",
    "def tokenizer(s: str):\n",
    "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "global_vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # record the prior\n",
    "# with torch.no_grad():\n",
    "#     bert_input = (self.token_indexers[\"tokens\"]\n",
    "#                   .tokens_to_indices(input_toks, global_vocab, \"tokens\"))\n",
    "#     token_ids = torch.LongTensor(bert_input[\"tokens\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from allennlp.data import DatasetReader, Instance, Token\n",
    "from allennlp.data.fields import (TextField, SequenceLabelField, LabelField, \n",
    "                                  MetadataField, ArrayField)\n",
    "\n",
    "class LongArrayField(ArrayField):\n",
    "    @overrides\n",
    "    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.Tensor:\n",
    "        tensor = torch.from_numpy(self.array)\n",
    "        return tensor\n",
    "    \n",
    "class FloatArrayField(ArrayField):\n",
    "    @overrides\n",
    "    def as_tensor(self, padding_lengths: Dict[str, int]) -> torch.FloatTensor:\n",
    "        tensor = torch.FloatTensor(self.array)\n",
    "        return tensor\n",
    "\n",
    "class DebiasingDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer, token_indexers, \n",
    "                 prior_precomputed: bool=False) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers\n",
    "        self.vocab = token_indexers[\"tokens\"].vocab\n",
    "        self._prior_precomputed = prior_precomputed\n",
    "\n",
    "    def _proc(self, x):\n",
    "        if x == \"[MASK]\": return x\n",
    "        else: return x.lower()\n",
    "        \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[str], w1: str, w2: str, \n",
    "                         p1: Optional[float], p2: Optional[float]) -> Instance:\n",
    "        fields = {}\n",
    "        input_toks = [Token(self._proc(x)) for x in tokens]\n",
    "        fields[\"input\"] = TextField(input_toks, self.token_indexers)        \n",
    "        # take [CLS] token into account\n",
    "        mask_position = tokens.index(\"[MASK]\") + 1\n",
    "        fields[\"mask_positions\"] = LongArrayField(\n",
    "            np.array(mask_position, dtype=np.int64),\n",
    "         )\n",
    "        fields[\"target_ids\"] = LongArrayField(np.array([\n",
    "            self.vocab[w1], self.vocab[w2],\n",
    "        ], dtype=np.int64))\n",
    "                \n",
    "        if self._prior_precomputed:\n",
    "            fields[\"prior_prob_sum\"] = FloatArrayField(np.array(p1 + p2, dtype=np.float32))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                bert_input = (self.token_indexers[\"tokens\"]\n",
    "                              .tokens_to_indices(input_toks, global_vocab, \"tokens\"))\n",
    "                token_ids = torch.LongTensor(bert_input[\"tokens\"]).unsqueeze(0)\n",
    "                probs = masked_lm(token_ids)[0, mask_position, :].detach().numpy()\n",
    "                probs = (probs - probs.max())\n",
    "                probs = probs.exp() / probs.exp().sum()\n",
    "                fields[\"prior_prob_sum\"] = \\\n",
    "                    FloatArrayField(np.array(probs[self.vocab[w1]] + probs[self.vocab[w2]],\n",
    "                               dtype=np.float32))\n",
    "            \n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        p1, p2 = 0., 0.\n",
    "        with open(file_path, \"rt\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                if self._prior_precomputed: sentence, w1, w2, p1, p2 = row\n",
    "                else: sentence, w1, w2 = row\n",
    "                yield self.text_to_instance(\n",
    "                    self.tokenizer(sentence), \n",
    "                    w1, w2, # words\n",
    "                    float(p1), float(p2), # prior probs\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 16097.23it/s]\n",
      "10000it [00:00, 18546.61it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = DebiasingDatasetReader(tokenizer=tokenizer, \n",
    "                                token_indexers={\"tokens\": token_indexer},\n",
    "                                prior_precomputed=config.prior_precomputed)\n",
    "train_ds, val_ds = (reader.read(DATA_ROOT / fname) for fname in [\"sample_w_probs.csv\", \"sample_w_probs.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator\n",
    "\n",
    "iterator = BucketIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        biggest_batch_first=config.testing,\n",
    "        sorting_keys=[(\"input\", \"num_tokens\")],\n",
    "    )\n",
    "iterator.index_with(global_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'tokens': tensor([[  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102],\n",
       "          [  101,   103,  2003,  1037,  2160, 28478,   102]]),\n",
       "  'tokens-offsets': tensor([[1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5],\n",
       "          [1, 2, 3, 4, 5]]),\n",
       "  'tokens-type-ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'mask': tensor([[1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1]])},\n",
       " 'mask_positions': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'target_ids': tensor([[2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016],\n",
       "         [2002, 2016]]),\n",
       " 'prior_prob_sum': tensor([0.6395, 0.6395, 0.6395, 0.6395, 0.6395, 0.6395, 0.6395, 0.6395, 0.6395,\n",
       "         0.6395, 0.6395, 0.6395, 0.6395, 0.6395, 0.6395, 0.6395])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(x, y): return ((x - y) ** 2).mean()\n",
    "def mae_loss(x, y): return (x - y).abs().mean()\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self, margin: float=0.1):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, x, y):\n",
    "        return torch.relu((x - y).abs().mean() - self.margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_likelihood(logits, target_logits) -> torch.FloatTensor:\n",
    "    max_logits = logits.max(1, keepdim=True)[0]\n",
    "    log_exp_sum_logits = (logits - max_logits).exp().sum(1).log()\n",
    "    log_exp_sum_correct_logits = (target_logits - max_logits).exp().sum(1).log()\n",
    "    return log_exp_sum_logits - log_exp_sum_correct_logits\n",
    "\n",
    "def likelihood(logits, # (batch, V)\n",
    "               target_logits, # (batch, 2)\n",
    "               prior_prob_sum, # (batch, )\n",
    "     ):\n",
    "    \"\"\"log likelihood of either of the target ids being chosen\"\"\"\n",
    "    return _log_likelihood(logits, target_logits).mean()\n",
    "\n",
    "class Consistency(nn.Module):\n",
    "    def __init__(self, distance: Callable):\n",
    "        super().__init__()\n",
    "        self._distance = distance\n",
    "    \n",
    "    def forward(self, logits, # (batch, V)\n",
    "                target_logits, # (batch, 2)\n",
    "                prior_prob_sum, # (batch, )\n",
    "               ):\n",
    "        \"\"\"\n",
    "        Constrains prob sum put on two words to be roughly equal\n",
    "        TODO: Provide some probabilistic/statistical interpretation\n",
    "        \"\"\"\n",
    "        l = _log_likelihood(logits, target_logits)\n",
    "        return self._distance(l, prior_prob_sum.log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns the deviation of the log odds ratio from its desired value.\n",
    "    Denoting the probs as p and q there are several options available:\n",
    "        - MSE(log p, log q)\n",
    "        - Max-margin loss\n",
    "    TODO: Add option to set the optimal log odds ratio\n",
    "    TODO: Ensure the logits do not change significantly\n",
    "    \"\"\"\n",
    "    def __init__(self, loss_func: Callable=mae_loss,\n",
    "                 consistency_loss_func: Callable=likelihood,\n",
    "                 consistency_weight: float=1.):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self._consistency_loss = consistency_loss_func\n",
    "        self.consistency_weight = consistency_weight\n",
    "        \n",
    "    def forward(self, logits: torch.FloatTensor, # (batch, seq, V)\n",
    "                mask_positions: torch.LongTensor, # (batch, )\n",
    "                target_ids: torch.LongTensor, # (batch, 2)\n",
    "                prior_prob_sum: torch.FloatTensor, # (batch, )\n",
    "               ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        input_ids: Numericalized tokens\n",
    "        mask_position: Positions of mask tokens\n",
    "        target_ids: Ids of target tokens to compute log odds on\n",
    "        \"\"\"\n",
    "        bs, seq = logits.size(0), logits.size(1)\n",
    "\n",
    "        # Gather the logits for at the masked positions\n",
    "        # TODO: More efficient implementation?\n",
    "        # Gather copies the data to create a new tensor which we would rather avoid\n",
    "        sel = (mask_positions.unsqueeze(1)\n",
    "                .unsqueeze(2).expand(bs, 1, logits.size(2))) # (batch, 1, V)\n",
    "        logits_at_masked_positions = logits.gather(1, sel).squeeze(1) # (batch, V)\n",
    "        \n",
    "        # Gather the logits for the target ids\n",
    "        sel = target_ids\n",
    "        target_logits_at_masked_positions = logits_at_masked_positions.gather(1, sel).squeeze(1) # (batch, 2)\n",
    "        \n",
    "        bias_loss = self.loss_func(\n",
    "            target_logits_at_masked_positions[:, 0], # male logits\n",
    "            target_logits_at_masked_positions[:, 1], # female logits\n",
    "         )\n",
    "        consistency_loss = self._consistency_loss(\n",
    "            logits_at_masked_positions, \n",
    "            target_logits_at_masked_positions, # pass target logits\n",
    "            prior_prob_sum,\n",
    "         )\n",
    "        return bias_loss + consistency_loss * self.consistency_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The allennlp model (for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "\n",
    "class BERT(Model):\n",
    "    def __init__(self, vocab, bert_for_masked_lm, loss: nn.Module=BiasLoss()):\n",
    "        super().__init__(vocab)\n",
    "        self.bert_for_masked_lm = bert_for_masked_lm\n",
    "        self.loss = loss\n",
    "    \n",
    "    def forward(self, \n",
    "                input: TensorDict,\n",
    "                mask_positions: torch.LongTensor,\n",
    "                target_ids: torch.LongTensor,\n",
    "                prior_prob_sum: torch.FloatTensor,\n",
    "            ) -> TensorDict:\n",
    "        logits = self.bert_for_masked_lm(input[\"tokens\"])\n",
    "        out_dict = {\"loss\": self.loss(logits, mask_positions, \n",
    "                                      target_ids, prior_prob_sum)}\n",
    "        out_dict[\"logits\"] = logits\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_distance = mae_loss\n",
    "\n",
    "loss = BiasLoss(\n",
    "    loss_func=logit_distance,\n",
    "    consistency_loss_func=Consistency(logit_distance),\n",
    "    consistency_weight=config.consistency_weight,\n",
    ")\n",
    "model = BERT(global_vocab, masked_lm, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dict = dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(init_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias scores before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v: k for k, v in token_indexer.vocab.items()}\n",
    "\n",
    "def ttoi(t: str): return token_indexer.vocab[t]\n",
    "def itot(i: int): return rev_vocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm.eval()\n",
    "logits = masked_lm(processor.to_bert_model_input(\"[MASK] is a housemaid\"))[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.1598, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[ttoi(\"he\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.8144, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[ttoi(\"she\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    }
   ],
   "source": [
    "from allennlp.training import Trainer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    serialization_dir=None,\n",
    "    cuda_device=0 if torch.cuda.is_available() else -1,\n",
    "    num_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.1806 ||: 100%|██████████| 157/157 [11:31<00:00,  4.60s/it]\n",
      "loss: 1.0594 ||: 100%|██████████| 157/157 [02:57<00:00,  1.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 0,\n",
       " 'peak_cpu_memory_MB': 1209.479168,\n",
       " 'training_duration': '00:14:29',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 0,\n",
       " 'epoch': 0,\n",
       " 'training_loss': 1.180585659233628,\n",
       " 'training_cpu_memory_MB': 1209.479168,\n",
       " 'validation_loss': 1.059408180083439,\n",
       " 'best_validation_loss': 1.059408180083439}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(t): return t.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(arr):\n",
    "    if len(arr.shape) > 1:\n",
    "        return [to_words(a) for a in arr]\n",
    "    else:\n",
    "        arr = to_np(arr)\n",
    "        return \" \".join([itot(i) for i in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, batch: TensorDict):\n",
    "    return model(**batch)[\"logits\"].argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]',\n",
       " '[CLS] [MASK] is a house ##maid [SEP]']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_words(batch[\"input\"][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .',\n",
       " '. he she she he she .']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_words(get_preds(model, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm.eval()\n",
    "logits = masked_lm(processor.to_bert_model_input(\"[MASK] is a housemaid\"))[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.8591, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[ttoi(\"he\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.8306, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[ttoi(\"she\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PyTorch state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(masked_lm.state_dict(), MODEL_SAVE_DIR / \"state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Export as tensorflow checkpoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
