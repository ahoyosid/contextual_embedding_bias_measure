{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from bert_utils import Config, BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    "    subspace_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ContextWord:\n",
    "    sent: str\n",
    "    word: str\n",
    "    def __post_init__(self):\n",
    "        assert self.word in self.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(cword: ContextWord, use_last_mask=False):\n",
    "    sentence, word = cword.sent, cword.word\n",
    "    idx = processor.get_index(sentence, word, last=use_last_mask)\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        sequence_output, _ = model.bert(processor.to_bert_model_input(sentence),\n",
    "                                        output_all_encoded_layers=False)\n",
    "        sequence_output.squeeze_(0)\n",
    "        if outputs is None: outputs = torch.zeros_like(sequence_output)\n",
    "        outputs = sequence_output + outputs\n",
    "    return outputs.detach().cpu().numpy()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix(vecs):\n",
    "    sim_matrix = np.zeros((len(vecs), len(vecs)))\n",
    "    for i, v in enumerate(vecs):\n",
    "        for j, w in enumerate(vecs):\n",
    "            sim_matrix[i, j] = cosine_similarity(v, w)\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sim_matrix_df(sentences: List[str],\n",
    "                           words: List[str]):\n",
    "    sim = construct_sim_matrix([get_word_vector(ContextWord(sent, word)) for sent, word in zip(sentences, words)])\n",
    "    return pd.DataFrame(data=sim, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diff_similarity(cwords1, cwords2):\n",
    "    cword11, cword12 = cwords1\n",
    "    cword21, cword22 = cwords2\n",
    "    return cosine_similarity(get_word_vector(cword11) - get_word_vector(cword12),\n",
    "                             get_word_vector(cword21) - get_word_vector(cword22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = model.cls.predictions.decoder.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bias = model.cls.predictions.bias.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_logits(wv: np.ndarray) -> np.ndarray:\n",
    "    return model.cls(torch.FloatTensor(wv).unsqueeze(0)).detach().cpu().numpy()[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>man</th>\n",
       "      <th>woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390309</td>\n",
       "      <td>0.413819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.390309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.824558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.413819</td>\n",
       "      <td>0.824558</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer       man     woman\n",
       "programmer    1.000000  0.390309  0.413819\n",
       "man           0.390309  1.000000  0.824558\n",
       "woman         0.413819  0.824558  1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_sim_matrix_df([\"That person is a programmer.\", \n",
    "                         \"I am a man.\", \n",
    "                         \"I am a woman.\"],\n",
    "                       [\"programmer\", \"man\", \"woman\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15081199"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23052035"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31024465"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find gendered direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_vecs, female_vecs = [], []\n",
    "def add_word_vecs(s: str, male_w: str, female_w: str):\n",
    "    male_vecs.append(get_word_vector(ContextWord(s.replace(\"XXX\", male_w), male_w)))\n",
    "    female_vecs.append(get_word_vector(ContextWord(s.replace(\"XXX\", female_w), female_w)))\n",
    "\n",
    "for prof in [\"musician\", \"magician\", \"nurse\", \"doctor\", \"teacher\"]:\n",
    "    add_word_vecs(\"XXX is a YYY\".replace(\"YYY\", prof), \"he\", \"she\")\n",
    "    add_word_vecs(\"XXX works as a YYY\".replace(\"YYY\", prof), \"he\", \"she\")\n",
    "\n",
    "for action in [\"talk to\", \"hit\", \"ignore\", \"please\", \"remove\"]:\n",
    "    add_word_vecs(\"please YYY XXX\".replace(\"YYY\", action), \"him\", \"her\")\n",
    "    add_word_vecs(\"don't YYY XXX\".replace(\"YYY\", action), \"him\", \"her\")\n",
    "\n",
    "for thing in [\"food\", \"music\", \"work\", \"running\", \"cooking\"]:\n",
    "    add_word_vecs(\"XXX likes YYY\".replace(\"YYY\", thing), \"he\", \"she\")\n",
    "    add_word_vecs(\"XXX enjoys YYY\".replace(\"YYY\", thing), \"he\", \"she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_vecs = np.r_[male_vecs]\n",
    "female_vecs = np.r_[female_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def find_subspace(D: np.ndarray) -> PCA:\n",
    "    assert len(D.shape) == 2\n",
    "    pca = PCA(n_components=config.subspace_size)\n",
    "    return pca.fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = find_subspace(male_vecs - female_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a31f50438>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=np.arange(pca.n_components), y=pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it says in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the projection of a vector $ v $ onto $ B $ by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_B = \\sum_{j=1}^{k} (v \\cdot b_j) b_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word $ w \\in N $, let $ \\vec{w} $ be re-embedded to\n",
    "$$ \\vec{w} := \\vec{w} - \\vec{w_{B}} / || \\vec{w} - \\vec{w_{B}} || $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu := \\sum_{w \\in E}w / |E| $$\n",
    "$$ \\nu := \\mu - \\mu_B $$\n",
    "For each $ w \\in E $, \n",
    "$$ \\vec{w} := \\nu + \\sqrt{1 - ||\\nu||^2}\\frac{\\vec{w_B} - \\mu_B}{||\\vec{w_B} - \\mu_B||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subspace(X: np.ndarray, subspace: np.ndarray, norm=True) -> np.ndarray:\n",
    "    Xb = ((X @ subspace.T) @ subspace) # projection onto biased subspace\n",
    "    X = (X - Xb) / (np.linalg.norm(X - Xb))\n",
    "    if norm:\n",
    "        mu = X.mean(0)\n",
    "        mub = Xb.mean(0)\n",
    "        nu = mu - mub\n",
    "        return nu + np.sqrt(1 - nu**2) * (Xb - mub) / np.linalg.norm(Xb - mub)\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01650096, -0.1000125 , -0.00558285, ..., -0.03515185,\n",
       "        -0.1651776 ,  0.01506699],\n",
       "       [ 0.01348943, -0.10363556, -0.00717191, ..., -0.03128222,\n",
       "        -0.15948816,  0.02207921],\n",
       "       [ 0.01125072, -0.10663339, -0.00471461, ..., -0.02876034,\n",
       "        -0.15997502,  0.02352692],\n",
       "       ...,\n",
       "       [ 0.00804645, -0.11074347,  0.0040258 , ..., -0.02450902,\n",
       "        -0.16264895,  0.0224969 ],\n",
       "       [ 0.00983971, -0.11074638,  0.00490333, ..., -0.02531959,\n",
       "        -0.16378503,  0.02271097],\n",
       "       [ 0.00980121, -0.1096987 ,  0.00346164, ..., -0.02612087,\n",
       "        -0.16351649,  0.02183877]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_subspace(male_vecs, pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newly checking for differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Postprocess\"\"\"\n",
    "    return remove_subspace(np.expand_dims(X, 0), pca.components_, norm=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_diff_similarity(cwords1, cwords2):\n",
    "    cword11, cword12 = cwords1\n",
    "    cword21, cword22 = cwords2\n",
    "    return cosine_similarity(pp(get_word_vector(cword11)) - pp(get_word_vector(cword12)),\n",
    "                             pp(get_word_vector(cword21)) - pp(get_word_vector(cword22)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarities are being reduced, so there is a shared gender subspace to a certain extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.119350664"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14438437"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19711463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_new_diff_similarity(\n",
    "    (ContextWord(\"he likes sports.\", \"he\"), ContextWord(\"she likes sports.\", \"she\")),\n",
    "    (ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "     ContextWord(\"The nurse went to the office.\", \"nurse\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for change in bias score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the bias score decreases with this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_score(sentence: str, gender_words: Iterable[str], \n",
    "               word: str, gender_comes_first=True, \n",
    "               correct_bias=True,\n",
    "               postprocess=False) -> float:    \n",
    "    mw, fw = gender_words\n",
    "    mwi, fwi = processor.token_to_index(mw), processor.token_to_index(fw)\n",
    "    wv = get_word_vector(\n",
    "        ContextWord(sentence.replace(\"XXX\", word).replace(\"GGG\", \"[MASK]\"), \"[MASK]\"),\n",
    "        use_last_mask=not gender_comes_first,        \n",
    "    )\n",
    "    if postprocess: wv = pp(wv)\n",
    "    logits = to_logits(wv)\n",
    "    subject_fill_bias = logits[mwi] - logits[fwi]\n",
    "    if correct_bias:\n",
    "        wv = get_word_vector(\n",
    "            ContextWord(sentence.replace(\"XXX\", \"[MASK]\").replace(\"GGG\", \"[MASK]\"), \"[MASK]\"),\n",
    "            use_last_mask=gender_comes_first,\n",
    "        )\n",
    "        if postprocess: wv = pp(wv)\n",
    "        prior_logits = to_logits(wv)\n",
    "        prior_bias = prior_logits[mwi] - prior_logits[fwi]\n",
    "        subject_fill_bias = subject_fill_bias - prior_bias\n",
    "    return subject_fill_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is amplified here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32679677"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"doctor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6350299"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"doctor\", postprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is neutralized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.859275"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"nurse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26979792"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is a XXX.\", [\"he\", \"she\"], \"nurse\", postprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4315097"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42505538"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"beautiful\", postprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is neutralized despite being biased in male direction before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4758742"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"dangerous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2890259"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"dangerous\", postprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.32514405"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19705546"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG is very XXX.\", [\"he\", \"she\"], \"cute\", postprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unintended Side Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any unintended side effects of this transformation? Let's test and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0094642835"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "     ContextWord(\"The doctor went to the office.\", \"doctor\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " def construct_sim_matrix_df(cws: List[ContextWord]):\n",
    "    return pd.DataFrame(data=sim, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683078</td>\n",
       "      <td>0.634901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>0.683078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>0.634901</td>\n",
       "      <td>0.835144</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer    doctor     nurse\n",
       "programmer    1.000000  0.683078  0.634901\n",
       "doctor        0.683078  1.000000  0.835144\n",
       "nurse         0.634901  0.835144  1.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cws = [\n",
    "    ContextWord(\"The programmer went to the office.\", \"programmer\"),\n",
    "    ContextWord(\"The doctor went to the office.\", \"doctor\"),\n",
    "    ContextWord(\"The nurse went to the office.\", \"nurse\"),\n",
    "]\n",
    "sim = construct_sim_matrix([get_word_vector(cw) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the similarities here seem to be roughly preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>programmer</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>programmer</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681834</td>\n",
       "      <td>0.634544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>0.681834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>0.634544</td>\n",
       "      <td>0.836736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            programmer    doctor     nurse\n",
       "programmer    1.000000  0.681834  0.634544\n",
       "doctor        0.681834  1.000000  0.836736\n",
       "nurse         0.634544  0.836736  1.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = construct_sim_matrix([pp(get_word_vector(cw)) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>dangerous</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600859</td>\n",
       "      <td>0.583575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangerous</th>\n",
       "      <td>0.600859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.583575</td>\n",
       "      <td>0.536445</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           beautiful  dangerous    normal\n",
       "beautiful   1.000000   0.600859  0.583575\n",
       "dangerous   0.600859   1.000000  0.536445\n",
       "normal      0.583575   0.536445  1.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cws = [\n",
    "    ContextWord(\"Your colleague is very beautiful.\", \"beautiful\"),\n",
    "    ContextWord(\"Your colleague is very dangerous.\", \"dangerous\"),\n",
    "    ContextWord(\"Your colleague is very normal.\", \"normal\"),\n",
    "]\n",
    "sim = construct_sim_matrix([get_word_vector(cw) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>dangerous</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576893</td>\n",
       "      <td>0.570956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dangerous</th>\n",
       "      <td>0.576893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.570956</td>\n",
       "      <td>0.515323</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           beautiful  dangerous    normal\n",
       "beautiful   1.000000   0.576893  0.570956\n",
       "dangerous   0.576893   1.000000  0.515323\n",
       "normal      0.570956   0.515323  1.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = construct_sim_matrix([pp(get_word_vector(cw)) for cw in cws])\n",
    "pd.DataFrame(data=sim, index=[cw.word for cw in cws], columns=[cw.word for cw in cws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linearity of BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT embeddings no longer express the same linear semantics as word2vec/GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57149196"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"I am a king.\", \"king\"),\n",
    "     ContextWord(\"I am a queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4555073"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"he is my friend.\", \"he\"), ContextWord(\"she is my friend.\", \"she\")),\n",
    "    (ContextWord(\"they are the king.\", \"king\"),\n",
    "     ContextWord(\"they are the queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in direction does not stay constant as the subject/object status of the word changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35181317"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"The king walked across the road.\", \"king\"),\n",
    "     ContextWord(\"The queen walked across the road.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24944247"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"king does not do such things.\", \"king\"),\n",
    "     ContextWord(\"queen does not do such things\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40263602"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"I captured the opponent's king.\", \"king\"),\n",
    "     ContextWord(\"I captured the opponent's queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21260612"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"I am a man.\", \"man\"), ContextWord(\"I am a woman.\", \"woman\")),\n",
    "    (ContextWord(\"king, please forgive me!\", \"king\"),\n",
    "     ContextWord(\"queen, please forgive me!\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44601032"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_similarity(\n",
    "    (ContextWord(\"his attitude is irritating.\", \"his\"), \n",
    "     ContextWord(\"her attitude is irritating.\", \"her\")),\n",
    "    (ContextWord(\"they are the king.\", \"king\"),\n",
    "     ContextWord(\"they are the queen.\", \"queen\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
